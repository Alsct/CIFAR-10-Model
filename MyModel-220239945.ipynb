{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5d09c-366a-4139-a22c-855a8ee31ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# stem class, outputs low level features straight from the batch input\n",
    "# consists of a convolution layer followed by batch normalization and ReLU activation.\n",
    "# normalise to make the output easier to process, relu adding no linearity to get more complex feature\n",
    "# THIS CODE IS ADAPTED FROM WEEK 7 ECS659U TUTORIAL SHEET \"conv_example.ipynb\"\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x # output is [B, C, 32, 32]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Expert branch creates the weights need to multiply by the corresponding convk\n",
    "# uses global avg feature pooling, followed by two fully connected layers\n",
    "# input from stem or previous block = [B, C, W, H]\n",
    "# output = [B, K]\n",
    "# THIS CODE IS ALSO ADAPTED FROM WEEK 7 ECS659U TUTORIAL SHEET \"conv_example.ipynb\"\n",
    "class ExpertBranch(nn.Module):\n",
    "    def __init__(self, channels, reduction, K):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1) #compress spatial info via avg -> [B, C, 1, 1]\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction) # first fully connected layer, as required reduce channels by a factor of R in my case reduction\n",
    "        self.fc2 = nn.Linear(channels // reduction, K) # as required this produce -> [B, K]\n",
    "        self.relu = nn.ReLU(inplace=True) # this represents g from the coursework outline, this adds non linearity to get more complex feature maps\n",
    "        self.softmax = nn.Softmax(dim=1) # soft max to sum to one\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, start_dim=1) # flattern the dimensions from [B, C, 1, 1] -> [B, C] needed as linear needs 2d\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x) # here dimensions go from [B, C] -> [B, K]\n",
    "        a = self.softmax(x) # each batch sums to 1\n",
    "        return a # [B, K]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create K convolutional layers with added complexity, each layer includes BatchNorm and ReLU activation\n",
    "class ConvLayers(nn.Module):\n",
    "    def __init__(self, channels, K):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(channels), # normalise features to make the output easier to process\n",
    "                nn.ReLU(inplace=True)  # adds non linearity allowing the network to learn more complex functions\n",
    "            )\n",
    "            for _ in range(K) # repeat for k times\n",
    "        ])\n",
    "\n",
    "    #takes in input x which is either the output of the stem, or the output of a previous block, dimensions [B, C, H, W]\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for conv in self.convs: # go through each conv layer in the convs module list\n",
    "            out = conv(x)   # apply each conv path to the same input\n",
    "            outputs.append(out)  # collect all outputs into a list\n",
    "        return outputs  # return list of [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# single block, block consists of expert layer and 'K' conv layers\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, channels, reduction, K):\n",
    "        super().__init__()\n",
    "        self.expert = ExpertBranch(channels, reduction, K)\n",
    "        self.conv_layers = ConvLayers(channels, K)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        a = self.expert(x) #output of expert branch is [b, k]\n",
    "        conv_outs = self.conv_layers(x)\n",
    "        weighted_sum = 0\n",
    "        # conv_out shape [B, C, H, W ]\n",
    "        # ai shape is [b,]\n",
    "        for i, conv_out in enumerate(conv_outs): # for every batches ak multiply it by the corresponding conv layers output convk\n",
    "            weight = a[:, i].view(-1, 1, 1, 1) #reshape ai so that it can be multiplied by conv_out, new shape [B, 1, 1, 1]\n",
    "            weighted_sum += weight * conv_out\n",
    "        \n",
    "        out = self.relu(weighted_sum) # add non linearity \n",
    "        \n",
    "        return out # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the backbone consists of 'n' number of blocks, I have defined 'n' as 'num_blocks'\n",
    "# Each block takes input from the output of the previous block, the first blocks takes input from the stem.\n",
    "# this implementation of the backbone was inspired by https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, num_blocks, channels, reduction, K):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([        #create a list of 'n' block modules         \n",
    "            Block(channels, reduction, K) for _ in range(num_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x) #the output of a block is the input for the next block\n",
    "        return x #output of the backbone is the result of going through each block\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the final high level features after all the blocks, takes in a input of [B, C, H, W]\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, channels, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1) # reduces each feature map to a single value which is the avg\n",
    "        self.fc = nn.Linear(channels, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x) #outputs [B, C, 1, 1]\n",
    "        x = torch.flatten(x, start_dim=1) # flattern to [B, C]\n",
    "        return self.fc(x) # output is [B, classes] logits, no softmax called in classifier as its called in the train one epoch, called once cross enthropy is called\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# main Model class, intialise the stem, backbone and classifier\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_blocks, in_channels, output_channels, reduction, K, num_classes):\n",
    "        super().__init__()\n",
    "        self.stem = Stem(in_channels, output_channels)  \n",
    "        self.backbone = Backbone(num_blocks, output_channels, reduction, K)\n",
    "        self.classifier = Classifier(output_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x) # the stem gets the input tensor 'x' dimensions [batchsize, input_channels, height, width], returns tensor size [batchsize, outchannels, 32, 32]\n",
    "        x = self.backbone(x) # takes in the output of the stem, [batchsize, outchannels, 32, 32]\n",
    "        x = self.classifier(x)# takes in the output of the backbones\n",
    "        return x #logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loads the CIFAR-10 dataset with data augmentation for training\n",
    "# for the normalize values i used https://github.com/kuangliu/pytorch-cifar/issues/19 , found online, specific for cifar 10.\n",
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "def load_data(batch_size, num_workers):\n",
    "    transform_train = transforms.Compose([      #for the augmentations I used the same  https://juliusruseckas.github.io/ml/lightning.html\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "    # load cifar 10 datasets with transforms\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    # wrap in DataLoaders for batching and shuffling\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#return the loss function, optimiser and scheduler\n",
    "def get_lossfunc_optimiser_scheduler(model, lr, weight_decay, num_epochs):\n",
    "    lossfunc = nn.CrossEntropyLoss() # using softmax regression for classifier\n",
    "    optimiser = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) # decided on adamW as i use weight decay, says adam is not great with weight decay https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=num_epochs) # gradually reduce learning rate, so if i chose a bad lr it can adapt\n",
    "    return lossfunc, optimiser, scheduler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "adapted train_one_epoch and evaluate from these\n",
    "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "https://discuss.pytorch.org/t/on-running-loss-and-average-loss/107890\n",
    "\"\"\"\n",
    "def train_one_epoch(model, train_loader, lossfunc, optimiser, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0 # correct predictions\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimiser.zero_grad() # clear gradients from previous step\n",
    "        outputs = model(images) # forward pass\n",
    "        loss = lossfunc(outputs, labels) # compute loss\n",
    "        loss.backward() # backward pass\n",
    "        optimiser.step() # update weights\n",
    "\n",
    "        running_loss += loss.item() * images.size(0) # accumulate total loss across images accounts for varying batch sizes\n",
    "\n",
    "        _, predicted = outputs.max(1) # get class with highest score\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item() # count correct predictions\n",
    "\n",
    "    return running_loss / total, 100.0 * correct / total # return avg loss and accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# getting test accuracy\n",
    "def evaluate(model, test_loader, lossfunc, device):\n",
    "    model.eval() # set model to eval instead  of train\n",
    "    running_loss = 0.0\n",
    "    correct = 0 # correct predictions\n",
    "    total = 0\n",
    "    with torch.no_grad(): # disable gradient comp\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = lossfunc(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, predicted = outputs.max(1) # get class with highest score\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / total, 100.0 * correct / total  # return avg loss and accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "trains and evaluates a CNN on the cifar 10 dataset\n",
    "\n",
    "sets up data, model, loss, optimiser, and scheduler. Runs training for multiple epochs,\n",
    "logs accuracy and loss, and plots performance metrics\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    train_loader, test_loader = load_data(batch_size=64, num_workers=2)\n",
    "    num_epochs = 100\n",
    "\n",
    "    \n",
    "    model = Model(num_blocks=8, in_channels=3, output_channels=64, reduction=8, K=3, num_classes=10).to(device)\n",
    "    lossfunc, optimiser, scheduler = get_lossfunc_optimiser_scheduler(model, lr=0.0003, weight_decay=0.001, num_epochs=100)\n",
    "\n",
    "\n",
    "    train_acc_history, test_acc_history, train_loss_history, test_loss_history = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, lossfunc, optimiser, device)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, lossfunc, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        test_loss_history.append(test_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        test_acc_history.append(test_acc)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% \"\n",
    "              f\"Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}% Time: {duration:.2f}s\")\n",
    "\n",
    "    #graphs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_acc_history, label='Train Accuracy')\n",
    "    plt.plot(test_acc_history, label='Test Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(test_loss_history, label='Test Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209d23d-b936-477a-a3c2-b3317d269f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51e938-99ec-4016-91d9-eb18a855a224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
